# Data Manipulation with Base R

Manipulating data has always been a core use case of R and its predecessor S. However, most R users currently rely on contributed packages like `dplyr` and `data.table` to perform basic data manipulation tasks, including **filtering**, **transforming**, **sorting**, **splitting**, **combining** and **summarizing**. Ideally, base R would provide the standard means of performing all of those operations; however, a few shortcomings lead to the development and adoption of alternative frameworks. This document reflects on those shortcomings and how they might be addressed while minimizing breaking changes.

A big reason for the success of R is that its data structures are designed for data analysis. All data are stored in vectors, and vectors are very usefully combined into a tabular data structure known as the `data.frame`, which represents a dataset and could be considered the primary data structure of R. Readability and convenience are two of the most important considerations when implementing a data workflow, and it is to the benefit of both to express workflows as a sequence of endomorphic operations on the `data.frame`. For that to be possible, each of the basic operations, those bolded above, need to accept and return a `data.frame` or, sometimes, a collection of `data.frame`s. Performance is often cited in the comparison of data manipulation frameworks; however, while performance is a convenient metric due to its ease of quantification, it is likely a distant third to readability and convenience when considered by the average, pragmatic R user.

R does support for some endomorphic operations on `data.frame`s, including:

- **Filtering:** `subset()`
- **Transforming:** `transform()`
- **Splitting:** `split()`
- **Combining:** `rbind()`, `cbind()`, `merge()`, `array2DF()`
- **Summarizing:** `aggregate()`

The presence of these functions indicates that endomorphic data pipelines were recognized as important, as far back as S. However, some of these have significant limitations relative to competing frameworks, and notably missing from the list is support for sorting. Only `subset()` and `transform()` are really competitive with their analogs in competing frameworks. The rest of this document treats the limitations and opportunities for improvement of the base R support for the remainder of the operations.

## Sorting

Sorting is a fundamental operation in data analysis, and it is surprising that base R does not provide a dedicated function for sorting `data.frame`s. While it is possible to sort a `data.frame` using the `order()` function in conjunction with indexing, this approach is not as intuitive or readable as a dedicated sorting function would be. Calling `order()` breaks the endopmorphic flow by requiring the user to step outside of the `data.frame` context. Also, passing column variables to `order()` can be cumbersome and error-prone.

Adding a `sort.data.frame()` method to base R would greatly enhance the usability of the language for data manipulation tasks. This function could accept a `data.frame` and a formula specifying one or more column names (or indices) to sort by, along with options for ascending/descending order. The function would return a sorted `data.frame`, maintaining the endomorphic flow.

## Splitting

The existing `split()` function in base R is quite powerful and flexible, allowing users to split a `data.frame` into a list of `data.frame`s based on the values of one or more grouping variables, specified as a formula. However, the output of `split()` is a list, which can be less convenient to work with in an endomorphic pipeline. It would be more convenient to have a data structure that retains the grouping information in a more structured way and supports transformations and aggregations directly, without requiring the user to think of the data as a list that has to be applied over. The closest analog is the return value of `tapply()`, which is an array with `@dimnames` that encapsulate the grouping structure. We could make the `FUN=` argument to `by()` default to `identity`, so that `by(df, ~ group)` becomes an intuitive way to split data into groups. Various base R primitives, such as those in `Arith()` and `Summary()`, as well as dataset-level operations like `subset()`, `transform()` and `aggregate()`, could be extended to operate on such list arrays (and even plain lists), allowing for seamless transformations and aggregations, before reassembling the data with `array2DF()`.  To improve performance, the ALTVEC backend could be leveraged to represent virtually grouped data without the overhead of creating multiple objects. This approach is still more complicated than the alternatives provided by `dplyr` and `data.table`, but it would be a significant improvement over the current state of base R. The S4Vectors package did this for one dimensional groupings as lists, and similar techniques could be applied here for the more general case.

## Combining

There are two modes of combining datasets: concatenation and joining. As long as the data are in a (re)combinable form, both modes are rather straightforward in base R using functions like `rbind()`, `cbind()`, `array2DF()`, and `merge()`. There are some gaps, however. Even after the introduction of a `group()` function, it will still be common to have lists of objects to combine. While `do.call(rbind, ...)` and `do.call(cbind, ...)` are serviceable for this purpose, they are not as user-friendly as dedicated functions like `dplyr::bind_rows()` and `dplyr::bind_cols()`. It might be possible to extend `array2DF()` to accept lists, providing a more convenient way to combine multiple datasets (unfortunately `list2DF()` is already taken for another purpose). It would also be nice to have a verb alias for `array2DF()` that is more intuitive for combining datasets. We could either introduce a new function, say `combine()`, or generalize `stack()` to support both lists of `data.frame`s and arrays in general.

## Summarizing

The existing `aggregate()` function in base R is quite powerful and flexible, allowing users to summarize data by one or more grouping variables, specified as a formula. However, the syntax can be somewhat cumbersome, especially when compared to the more intuitive syntax provided by `dplyr::summarize()`. Additionally, `aggregate()` does not support the use of multiple summary functions in a single call, which can be a limitation for users who want to compute multiple summaries simultaneously. One approach would be to allow `FUN=` to be missing, in which case the arguments in `...` are evaluated in the context of the input `data.frame`, except the variables are implicitly split into lists by `by=` and evaluation depends on the list-level operations proposed above. There could also be a new method for the list arrays discussed earlier that would omit the `by=` argument.

# Conclusion

The changes we propose vary in cost and benefit. Adding a `sort.data.frame()` is low-hanging fruit, while enabling basic data manipulation functions to operate on lists would be much more complex but potentially rewarding. All of the changes could be made in package space first, allowing for experimentation and refinement before being considered for inclusion in base R, even if it means overriding some base functions. By addressing these shortcomings, base R could provide a more comprehensive and user-friendly set of tools for writing fluent data workflows without introducing entirely new constructs, thus improving the cohesion of the language.